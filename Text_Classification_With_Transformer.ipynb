{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jCY7lYq5X7Xd"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING TRANSFORMER BLOCK"
      ],
      "metadata": {
        "id": "QAz45cs-YN2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  # will contain all our layers that happens insider the block\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "      #embed dim : size of EMbedding vector\n",
        "      # numhead - number of multi head attentions\n",
        "      # ff -> feed forward neural network\n",
        "      # rate -> for dropout\n",
        "        super().__init__()\n",
        "        #multi head Attention layer\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "       # nn\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        #2 layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        #dropout to be used later\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      # apply multi head self attention\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        # apply droput in it\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        # adding layer Normalizatin\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        #neural netword with dropout\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        # layer normalization\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "enFRiLShYDZx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding and TOkenization"
      ],
      "metadata": {
        "id": "JgV7e9rjZi7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # learning new token embedding\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        #position encoding -> sequence in the token encoding\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "        #convert token IDS to token vectors"
      ],
      "metadata": {
        "id": "Vu0jmcRdZf6b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "4gDIqqHMaQqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "#max length of sentence=200\n",
        "maxlen = 200\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJfk1jk6aOWS",
        "outputId": "5d7d7d7b-17db-453a-d0b4-58cf68c3d322"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PADDING\n",
        "\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "FfoUWfzoinj0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "bDZes_xgarbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim=32 # embedding size for each token\n",
        "num_heads=2 # nom of attention heads\n",
        "ff_dim=32 # 32 hidden layer size in feed forward network"
      ],
      "metadata": {
        "id": "o6_pGGMvaoQL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=layers.Input(shape=(maxlen,))\n",
        "embedding_layer=TokenAndPositionEmbedding(maxlen,vocab_size,embed_dim)\n",
        "#setting embedding layer"
      ],
      "metadata": {
        "id": "IJZKpURwa5_t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "u7Y5mhJWbjpP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = embedding_layer(inputs)\n",
        "# applying token and position Embedding layer\n",
        "#converted words into token then positional tokens.\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "#applying transformer block\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "#reduce sequence to single vector per sample\n",
        "x = layers.Dropout(0.1)(x)\n",
        "#nn\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "#predicting the 1 word\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "fOh5iYcrbFFH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "-AdLPI79c-1r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi9EzAU9dJbX",
        "outputId": "7b94534d-9616-47f3-e426-764eb00d6f88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 80ms/step - accuracy: 0.6945 - loss: 0.5385 - val_accuracy: 0.8798 - val_loss: 0.2849\n",
            "Epoch 2/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 78ms/step - accuracy: 0.9274 - loss: 0.1945 - val_accuracy: 0.8739 - val_loss: 0.3019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7xlZ5BjdMOm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}